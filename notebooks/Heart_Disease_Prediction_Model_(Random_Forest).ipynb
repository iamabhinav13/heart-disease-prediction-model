{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g4wa9DiP2AJ_",
    "outputId": "4d94f1b4-9bcc-4aa1-a48a-d328267541ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading and merging data...\n",
      "Data loaded and merged successfully!\n",
      "Final dataset shape: (253688, 25)\n",
      "--------------------------------------------------\n",
      "Step 2: Preparing data for modeling...\n",
      "\n",
      "Target Variable Distribution:\n",
      "HeartDiseaseorAttack\n",
      "0    229795\n",
      "1     23893\n",
      "Name: count, dtype: int64\n",
      "Training set size: 202950 samples\n",
      "Testing set size: 50738 samples\n",
      "--------------------------------------------------\n",
      "Step 3: Training Random Forest model and tuning hyperparameters...\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "\n",
      "Hyperparameter tuning complete!\n",
      "Best hyperparameters found:\n",
      "{'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 10, 'bootstrap': True}\n",
      "--------------------------------------------------\n",
      "Step 4: Evaluating the best model on the test set...\n",
      "\n",
      "PERFORMANCE METRICS:\n",
      "====================\n",
      "Accuracy: 0.7427\n",
      "ROC-AUC Score: 0.8220\n",
      "\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "No Heart Disease       0.97      0.74      0.84     45959\n",
      "   Heart Disease       0.23      0.75      0.35      4779\n",
      "\n",
      "        accuracy                           0.74     50738\n",
      "       macro avg       0.60      0.74      0.60     50738\n",
      "    weighted avg       0.90      0.74      0.79     50738\n",
      "\n",
      "Confusion Matrix:\n",
      "[[34115 11844]   <-- Predicted No Heart Disease\n",
      " [ 1210  3569]]  <-- Predicted Heart Disease\n",
      "--------------------------------------------------\n",
      "Top 10 Most Important Features:\n",
      "                    importance\n",
      "GenHlth               0.188725\n",
      "HighBP                0.187491\n",
      "HighChol              0.140554\n",
      "DiffWalk              0.103027\n",
      "Stroke                0.059598\n",
      "Diabetes              0.051851\n",
      "Sex                   0.047076\n",
      "PhysHlth              0.044428\n",
      "Smoking_intensity     0.034418\n",
      "Income                0.033889\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for a cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "### =========================================================================\n",
    "### 1. LOAD AND MERGE DATA\n",
    "### =========================================================================\n",
    "# The first step is to combine all the separate data files into a single master dataframe.\n",
    "# We'll use the 'PatID' column as the common key for merging.\n",
    "\n",
    "print(\"Step 1: Loading and merging data...\")\n",
    "try:\n",
    "    demographic_df = pd.read_csv('demographic_table.csv')\n",
    "    patient_condition_df = pd.read_csv('patient_condition_table.csv')\n",
    "    smoking_df = pd.read_csv('smoking_status_table.csv')\n",
    "    alcohol_df = pd.read_csv('alcohol_use_table.csv')\n",
    "\n",
    "    # Sequentially merge dataframes. Using left merge to keep all patients\n",
    "    # from the demographic table and filling missing data for smokers/drinkers.\n",
    "    df = pd.merge(demographic_df, patient_condition_df, on='PatID', how='inner')\n",
    "    df = pd.merge(df, smoking_df, on='PatID', how='left')\n",
    "    df = pd.merge(df, alcohol_df, on='PatID', how='left')\n",
    "\n",
    "    # Fill NaN values that resulted from the left merge.\n",
    "    # We assume that if a patient has no record in the smoking or alcohol tables,\n",
    "    # they do not engage in those activities.\n",
    "    df['Smoking_Status'] = df['Smoking_Status'].fillna(0)\n",
    "    # The column name in the file has a trailing space\n",
    "    df['Smoking_intensity '] = df['Smoking_intensity '].fillna(0)\n",
    "    df['HvyAlcoholConsump'] = df['HvyAlcoholConsump'].fillna(0)\n",
    "\n",
    "    print(\"Data loaded and merged successfully!\")\n",
    "    print(f\"Final dataset shape: {df.shape}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Make sure all CSV files are in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "### =========================================================================\n",
    "### 2. DATA PREPARATION\n",
    "### =========================================================================\n",
    "# Here, we prepare the data for modeling. This involves defining our features\n",
    "# and target, and splitting the data into training and testing sets.\n",
    "\n",
    "print(\"Step 2: Preparing data for modeling...\")\n",
    "\n",
    "# Drop the patient identifier, as it's not a predictive feature\n",
    "df = df.drop('PatID', axis=1)\n",
    "\n",
    "# Correcting the target variable name based on the CSV file\n",
    "TARGET_VARIABLE = 'HeartDiseaseorAttack'\n",
    "\n",
    "# Define the feature matrix (X) and the target vector (y)\n",
    "X = df.drop(TARGET_VARIABLE, axis=1)\n",
    "y = df[TARGET_VARIABLE]\n",
    "\n",
    "# Check for class imbalance in the target variable\n",
    "class_counts = y.value_counts()\n",
    "print(\"\\nTarget Variable Distribution:\")\n",
    "print(class_counts)\n",
    "\n",
    "# Split data into 80% for training and 20% for testing\n",
    "# 'stratify=y' ensures the class distribution is the same in both train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "### =========================================================================\n",
    "### 3. MODEL TRAINING & HYPERPARAMETER TUNING (RANDOM FOREST)\n",
    "### =========================================================================\n",
    "# We will use RandomizedSearchCV to find the best settings for our RandomForest model.\n",
    "\n",
    "print(\"Step 3: Training Random Forest model and tuning hyperparameters...\")\n",
    "\n",
    "# Define the hyperparameter grid for RandomizedSearchCV to explore\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize RandomForest Classifier\n",
    "# class_weight='balanced' automatically adjusts weights inversely proportional to class frequencies\n",
    "rf_clf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Set up the Randomized Search with 5-fold cross-validation\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_clf,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=5,  # Number of parameter combinations to try\n",
    "    scoring='roc_auc',\n",
    "    cv=5,       # 5-fold cross-validation\n",
    "    verbose=1,\n",
    "    n_jobs=-1,  # Use all available CPU cores\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model to find the best hyperparameters\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract the best performing model from the search\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "print(\"\\nHyperparameter tuning complete!\")\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(random_search.best_params_)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "### =========================================================================\n",
    "### 4. MODEL EVALUATION\n",
    "### =========================================================================\n",
    "# Finally, we evaluate the tuned model on the unseen test data to see how\n",
    "# well it performs in predicting heart disease risk.\n",
    "\n",
    "print(\"Step 4: Evaluating the best model on the test set...\")\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1] # Probabilities for the positive class\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"\\nPERFORMANCE METRICS:\")\n",
    "print(\"=\"*20)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\\n\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No Heart Disease', 'Heart Disease']))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"[[{cm[0][0]:>5} {cm[0][1]:>5}]   <-- Predicted No Heart Disease\")\n",
    "print(f\" [{cm[1][0]:>5} {cm[1][1]:>5}]]  <-- Predicted Heart Disease\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Display the most important features driving the model's predictions\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "feature_importances = pd.DataFrame(\n",
    "    best_model.feature_importances_,\n",
    "    index=X_train.columns,\n",
    "    columns=['importance']\n",
    ").sort_values('importance', ascending=False)\n",
    "print(feature_importances.head(10))\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QxeaA5S8HuXx",
    "outputId": "fd37fd66-e8df-4917-e011-3beb7a36a119"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Saving the trained model...\n",
      "Model saved successfully as 'heart_disease_model.pkl'\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "### =========================================================================\n",
    "### 5. SAVE THE MODEL\n",
    "### =========================================================================\n",
    "# This final step saves the trained model to a file for deployment.\n",
    "# We use joblib as it is efficient for saving scikit-learn models.\n",
    "import joblib\n",
    "print(\"Step 5: Saving the trained model...\")\n",
    "\n",
    "model_filename = 'heart_disease_model.pkl'\n",
    "joblib.dump(best_model, model_filename)\n",
    "\n",
    "print(f\"Model saved successfully as '{model_filename}'\")\n",
    "print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
